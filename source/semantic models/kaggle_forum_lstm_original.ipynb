{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = 'data/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'cache/GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 211#np.random.randint(175, 275)\n",
    "num_dense = 120#np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.23#0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.32#0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "# word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "#         binary=True)\n",
    "# print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "unknown_words = []\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        unknown_words.append(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61789"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(embedding_matrix, axis=1) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['murwara',\n",
       " 'daiict',\n",
       " 'utnapishtim',\n",
       " 'sonja',\n",
       " 'devonians',\n",
       " 'hsv',\n",
       " 'fawx',\n",
       " 'paiza',\n",
       " 'turram',\n",
       " 'lottoland',\n",
       " '5987',\n",
       " 'chanthaburi',\n",
       " 'googlenet',\n",
       " 'shadowdancer',\n",
       " 'rickman',\n",
       " 'canem',\n",
       " 'suggesion',\n",
       " 'ecsta',\n",
       " 'tourister',\n",
       " 'kenpachi',\n",
       " 'zandikhohlisa',\n",
       " 'gazatted',\n",
       " 'freesteamgifts',\n",
       " 'broward',\n",
       " 'naturopathix',\n",
       " 'jrpg',\n",
       " 'throwaround',\n",
       " 'kaushal',\n",
       " 'vologda',\n",
       " 'sericin',\n",
       " 'ctk3200',\n",
       " '330ml',\n",
       " 'girokonto',\n",
       " 'thrace',\n",
       " '0051',\n",
       " '270',\n",
       " '271',\n",
       " '272',\n",
       " '273',\n",
       " '274',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '279',\n",
       " 'starboy',\n",
       " 'krampusnacht',\n",
       " 'naache',\n",
       " '4cell',\n",
       " '2300mhz',\n",
       " 'movietv',\n",
       " 'trivikram',\n",
       " '27i',\n",
       " '10700',\n",
       " 'bhubaneshwar',\n",
       " 'oct14',\n",
       " 't250si',\n",
       " 't250sl',\n",
       " 'motorolla',\n",
       " 'phillipine',\n",
       " '1670s',\n",
       " 'controverse',\n",
       " 'imporatnace',\n",
       " 'istributed',\n",
       " 'diomand',\n",
       " 'mclaurin',\n",
       " 'yaghan',\n",
       " 'sterk',\n",
       " 'ohhio',\n",
       " 'inlisted',\n",
       " 'was911really',\n",
       " 'sneha',\n",
       " 'issual',\n",
       " 'dne',\n",
       " 'dnd',\n",
       " 'thamirabarani',\n",
       " 'dnn',\n",
       " 'jspatch',\n",
       " 'lme',\n",
       " 'pgdhrm',\n",
       " 'unlearnable',\n",
       " 'twenty19',\n",
       " 'kayasthas',\n",
       " 'vasuda',\n",
       " 'inorgnc',\n",
       " 'avma',\n",
       " 'iraiva',\n",
       " 'stavara',\n",
       " '535i',\n",
       " 'borstar',\n",
       " 'buffallo',\n",
       " 'adventist',\n",
       " 'nativead',\n",
       " 'sy0',\n",
       " 'coprophilia',\n",
       " 'torqouise',\n",
       " 'oliveboard',\n",
       " 'chandrabati',\n",
       " 'pinth',\n",
       " 'tchaikovsky',\n",
       " 'ambala',\n",
       " 'dannon',\n",
       " '28teeth',\n",
       " 'essid',\n",
       " 'phoritto',\n",
       " 'shaitan',\n",
       " 'barbro',\n",
       " 'balade',\n",
       " 'censys',\n",
       " 'seda',\n",
       " 'bratislava',\n",
       " 'ouroboros',\n",
       " 'oritani',\n",
       " 'regstn',\n",
       " 'shreya',\n",
       " 'catracha',\n",
       " 'menlo',\n",
       " 'dlvrit',\n",
       " 'p207tx',\n",
       " 'surrat',\n",
       " 'seiya',\n",
       " 'boltons',\n",
       " 'uop',\n",
       " 'vectorize',\n",
       " 'uov',\n",
       " 'uot',\n",
       " 'whereis',\n",
       " 'cryolife',\n",
       " 'uoc',\n",
       " 'sizr',\n",
       " 'vrushali',\n",
       " '18v',\n",
       " 'shinawatra',\n",
       " 'fii',\n",
       " 'shivay',\n",
       " 'cdbaby',\n",
       " 'shivas',\n",
       " 'shivam',\n",
       " 'jamali',\n",
       " 'buuz',\n",
       " 'class11th',\n",
       " '9w1',\n",
       " 'pandodaily',\n",
       " 'moonmeander',\n",
       " 'mcgonagall',\n",
       " 'lucero',\n",
       " '9w8',\n",
       " 'vilom',\n",
       " 'raahaten',\n",
       " 'bartow',\n",
       " '598s',\n",
       " 'boyzone',\n",
       " 'cinemedia',\n",
       " 'abhimanyu',\n",
       " 'rabbitmq',\n",
       " 'thamizh',\n",
       " 'isyx',\n",
       " 'getch',\n",
       " 'quorarian',\n",
       " 'folarin',\n",
       " 'undecidable',\n",
       " 'picaroworld',\n",
       " 'synaptol',\n",
       " 'thacher',\n",
       " 'handside',\n",
       " 'sathyabhama',\n",
       " 'alagio',\n",
       " 'statusqo',\n",
       " 'partitiate',\n",
       " '6qn',\n",
       " 'religuous',\n",
       " 'khrap',\n",
       " 'angelfall',\n",
       " 'selfi',\n",
       " 'niammy',\n",
       " 'cougars69',\n",
       " 'bioshocks',\n",
       " 'farward',\n",
       " 'esophagal',\n",
       " 'katelynne',\n",
       " '90cm',\n",
       " 'helpbob',\n",
       " 'popularise',\n",
       " 'chromebooks',\n",
       " '18y',\n",
       " 'sfter',\n",
       " 'interract',\n",
       " 'trochaic',\n",
       " 'dgx',\n",
       " 'alwar',\n",
       " 'bilzerian',\n",
       " 'mastec',\n",
       " 'bockscar',\n",
       " 'travelblog',\n",
       " 'd5500',\n",
       " 'mastek',\n",
       " 'forevermark',\n",
       " 'caitlyn',\n",
       " 'phsically',\n",
       " 'rewardz',\n",
       " 'zeroth',\n",
       " 'evidaayirunnu',\n",
       " 'ahwazi',\n",
       " 'toip',\n",
       " 'sandshrew',\n",
       " 'condesending',\n",
       " 'duckworth',\n",
       " 'ahmet',\n",
       " 'biolgy',\n",
       " 'pogathey',\n",
       " 'being911able',\n",
       " '32gg',\n",
       " 'vnit',\n",
       " 'selassie',\n",
       " 'yeezys',\n",
       " 'dousunda',\n",
       " 'gorman',\n",
       " 'skeletool',\n",
       " 'medibang',\n",
       " 'bbnl',\n",
       " 'kuamari',\n",
       " 'nthan',\n",
       " '9750',\n",
       " 'tecphlie',\n",
       " 'domaincentral',\n",
       " 'mishri',\n",
       " 'farnham',\n",
       " 'vgn',\n",
       " 'mba2016',\n",
       " 'rhce',\n",
       " 'rhca',\n",
       " 'testsys',\n",
       " 'xat2017',\n",
       " 'vgp',\n",
       " 'vgs',\n",
       " '45th',\n",
       " 'dailed',\n",
       " '975l',\n",
       " 'sx376b',\n",
       " 'soemone',\n",
       " 'hso3',\n",
       " 'relpacing',\n",
       " 'thaad',\n",
       " 'thaai',\n",
       " 'teco',\n",
       " 'nominalization',\n",
       " 'na2so3',\n",
       " 'socho',\n",
       " 'spreecommerce',\n",
       " 'ajaya',\n",
       " 'wifikill2',\n",
       " 'incropera',\n",
       " '30335',\n",
       " 'xlvii',\n",
       " 'peco',\n",
       " 'azeotrope',\n",
       " 'caribs',\n",
       " 'puche',\n",
       " 'sturgess',\n",
       " 'khaasdaar',\n",
       " 'plath',\n",
       " 'firenze',\n",
       " 'terrorising',\n",
       " 'srabon',\n",
       " 'rtwapne',\n",
       " 'recyify',\n",
       " 'prulu',\n",
       " 'accomdation',\n",
       " '7560',\n",
       " 'hilali',\n",
       " 'adfurther',\n",
       " '10mm',\n",
       " 'aadam',\n",
       " 'subtills',\n",
       " 'pmfby',\n",
       " '27c',\n",
       " 'pinos',\n",
       " 'grailed',\n",
       " 'tyndall',\n",
       " 'untertitel',\n",
       " 'athar',\n",
       " 'chyavanprash',\n",
       " 'lotr',\n",
       " 'irr',\n",
       " 'irt',\n",
       " 'kumada',\n",
       " 'irm',\n",
       " 'srinivas',\n",
       " 'irb',\n",
       " 'gcet',\n",
       " 'ufa',\n",
       " 'guansen',\n",
       " '27m',\n",
       " '27n',\n",
       " '210205',\n",
       " 'papertronics',\n",
       " 'miyaki',\n",
       " 'diarrea',\n",
       " 'engility',\n",
       " 'ir6',\n",
       " 'acetophenone',\n",
       " 'i7559',\n",
       " 'numlk',\n",
       " '1month',\n",
       " 'skbuff',\n",
       " 'brokerlinking',\n",
       " 'placments',\n",
       " 'ozium',\n",
       " 'frc',\n",
       " 'frd',\n",
       " 'teewe',\n",
       " 'chrisian',\n",
       " 'jeor',\n",
       " 'kdenlive',\n",
       " 'badaruddin',\n",
       " 'amazonsmile',\n",
       " 'somatrophin',\n",
       " 'c2160',\n",
       " 'oclaro',\n",
       " 'sirasana',\n",
       " 'teatment',\n",
       " 'cocycle',\n",
       " 'bundelkhand',\n",
       " 'waddell',\n",
       " '5056',\n",
       " 'monowitz',\n",
       " 'recyclerview',\n",
       " '5051',\n",
       " 'coloneus',\n",
       " 'sellenium',\n",
       " 'morevisas',\n",
       " 'gbpuat',\n",
       " 'pllsss',\n",
       " 'i8552',\n",
       " 'prash',\n",
       " 'wothy',\n",
       " 'issac',\n",
       " '9days',\n",
       " 'qdoba',\n",
       " 'famoustylishnails',\n",
       " '1280x800',\n",
       " 'smothly',\n",
       " 'jagadhri',\n",
       " 'deliveey',\n",
       " 'charon',\n",
       " 'clips4sale',\n",
       " 'anupama',\n",
       " '1a111in',\n",
       " 'quanttative',\n",
       " '172cm',\n",
       " 'anxietydisorder',\n",
       " 'dtaa',\n",
       " 'watercolour',\n",
       " 'keynesians',\n",
       " 'endup',\n",
       " 'loudoun',\n",
       " 'for4000sq',\n",
       " 'freemyapps',\n",
       " 'churchofsatan',\n",
       " 'thibaut',\n",
       " 'spellathon',\n",
       " '24i',\n",
       " 'steri',\n",
       " 'photosin',\n",
       " 'darkon',\n",
       " 'bseb',\n",
       " 'lacroix',\n",
       " 'basanthali',\n",
       " 'ha1',\n",
       " 'newsnow',\n",
       " 'fancybox',\n",
       " 'ubmit',\n",
       " 'crowe',\n",
       " 'lesner',\n",
       " '8038',\n",
       " 'captiva',\n",
       " 'monsterusers',\n",
       " 'yoroshiku',\n",
       " 'jumeirah',\n",
       " 'ointernet',\n",
       " '568568568568',\n",
       " 'novasure',\n",
       " 'vidhur',\n",
       " 'unlinke',\n",
       " 'sentellas',\n",
       " 'anyboudy',\n",
       " 'insipidus',\n",
       " 'fyers',\n",
       " 'c4h10',\n",
       " 'bses',\n",
       " 'todefence',\n",
       " 'emberton',\n",
       " 'bhan',\n",
       " 'kotlin',\n",
       " 'crompton',\n",
       " 'acof',\n",
       " 'mollars',\n",
       " 'acos',\n",
       " 'sunpharma',\n",
       " 'mangoe',\n",
       " 'chijko',\n",
       " '30z',\n",
       " 'avalaible',\n",
       " 'iquanta',\n",
       " 'monitormojo',\n",
       " 'ferriday',\n",
       " '16000',\n",
       " 'ubercart',\n",
       " 'givewell',\n",
       " '5866',\n",
       " 'dadhichi',\n",
       " 'renuka',\n",
       " 'tokenize',\n",
       " 'kissflow',\n",
       " 'airlineprices',\n",
       " 'freebirds',\n",
       " 'horodecki',\n",
       " '1600m',\n",
       " '1600s',\n",
       " '1600w',\n",
       " 'bhau',\n",
       " 'barrymore',\n",
       " '64000',\n",
       " 'badmin',\n",
       " 'livepedia',\n",
       " 'posiible',\n",
       " 'synchronoss',\n",
       " 'wholesalefash',\n",
       " 'bodhidharma',\n",
       " 'hydroxydopamine',\n",
       " 'slideshare',\n",
       " '1080x2560',\n",
       " 'pasadena',\n",
       " 'regimine',\n",
       " 'transportmantra',\n",
       " 'crowne',\n",
       " 'iski',\n",
       " 'bossie',\n",
       " 'ubns',\n",
       " '1220s',\n",
       " 'chargebee',\n",
       " 'lmgtf',\n",
       " 'loret',\n",
       " 'phabplus',\n",
       " '303585',\n",
       " 'igbos',\n",
       " 'moresby',\n",
       " 'yellowstone',\n",
       " '6400m',\n",
       " 'aragorn',\n",
       " 'tartup',\n",
       " 'loren',\n",
       " 'neopentane',\n",
       " 'sanddornbalance',\n",
       " 'fukatsu',\n",
       " 'urgit',\n",
       " 'omran',\n",
       " 'omras',\n",
       " 'telecaller',\n",
       " 'crazyyy',\n",
       " '14maths',\n",
       " '12203',\n",
       " 'chail',\n",
       " 'mehrab',\n",
       " 'bluerock',\n",
       " 'chaid',\n",
       " 'darknight',\n",
       " 'entendre',\n",
       " 'machu',\n",
       " 'rabinder',\n",
       " 'anfis',\n",
       " 'datapower',\n",
       " 'favourite',\n",
       " 'misophonia',\n",
       " 'jeri',\n",
       " 'olympus',\n",
       " 'goodrec',\n",
       " 'klout',\n",
       " 'arshini',\n",
       " 'goodrem',\n",
       " 'juxt',\n",
       " 'hipparchus',\n",
       " 'tetrabutylammoniumbromide',\n",
       " 'mach1',\n",
       " '440mb',\n",
       " 'westcliff',\n",
       " 'amatriain',\n",
       " 'vanellope',\n",
       " 'unmix',\n",
       " 'lapierre',\n",
       " 'adorno',\n",
       " 'shreeman',\n",
       " 'abolisihed',\n",
       " 'yakumo',\n",
       " 'catelyn',\n",
       " 'papasamyam',\n",
       " 'forfishingvideos',\n",
       " 'matram',\n",
       " 'antwerp',\n",
       " 'nh2nh2',\n",
       " 'demonitasation',\n",
       " 'huepool',\n",
       " 'baggins',\n",
       " 'clashbot',\n",
       " 'mensutra',\n",
       " 'i257',\n",
       " 'frenectomy',\n",
       " '302',\n",
       " 'whitleys',\n",
       " 'focalin',\n",
       " '303',\n",
       " 'subbaraj',\n",
       " 'honour',\n",
       " 'webmoney',\n",
       " '306',\n",
       " 'kalewadi',\n",
       " 'hio4',\n",
       " 'roald',\n",
       " '3cscript',\n",
       " 'mulsim',\n",
       " 'mandula',\n",
       " 'studentlist',\n",
       " 'docstrings',\n",
       " 'wavii',\n",
       " 'giraud',\n",
       " 'dimitrova',\n",
       " 'shakeology',\n",
       " 'annalise',\n",
       " 'pularin',\n",
       " 'flightsiming',\n",
       " 'langara',\n",
       " 'innitwould',\n",
       " 'tayyibah',\n",
       " 'khurana',\n",
       " '3558u',\n",
       " 'vigour',\n",
       " 'insurence',\n",
       " 'osx86',\n",
       " 'mythomagic',\n",
       " 'alastair',\n",
       " 'bimtech',\n",
       " 'brexpiprazole',\n",
       " 'noresourceerror',\n",
       " 'pasquotank',\n",
       " 'fsx',\n",
       " 'nembutal',\n",
       " 'wvm',\n",
       " 'menaing',\n",
       " 'bharati',\n",
       " 'bharath',\n",
       " 'shyamali',\n",
       " 'simbalochan',\n",
       " 'rutherford',\n",
       " 'jadavpur',\n",
       " 'sworld',\n",
       " 'pushbullet',\n",
       " 'munroe',\n",
       " 'daliy',\n",
       " 'groupme',\n",
       " 'bodhidharman',\n",
       " 'camtasia',\n",
       " 'compresser',\n",
       " 'tadrishi',\n",
       " 'litre',\n",
       " 'wany',\n",
       " 'eiichiro',\n",
       " 'fretwell',\n",
       " 'upssc',\n",
       " 'yalom',\n",
       " 'teavana',\n",
       " 'falluja',\n",
       " 'cuhk',\n",
       " 'backpapers',\n",
       " 'denbury',\n",
       " 'unreciprocating',\n",
       " 'pollachi',\n",
       " 'bhisma',\n",
       " 'kahlon',\n",
       " 'mallaya',\n",
       " 'habiru',\n",
       " 'arpdau',\n",
       " 'jasprit',\n",
       " 'jungwirth',\n",
       " 'illycaff',\n",
       " 'tula',\n",
       " 'milken',\n",
       " 'illam',\n",
       " 'jio',\n",
       " 'jil',\n",
       " 'ahimsavadi',\n",
       " 'shibpur',\n",
       " 'jii',\n",
       " 'tulsidas',\n",
       " 'tulu',\n",
       " 'lshw',\n",
       " 'jalpaiguri',\n",
       " 'varifing',\n",
       " 'steves',\n",
       " 'forname',\n",
       " 'sortex',\n",
       " 'registan',\n",
       " '18562',\n",
       " 'thalia',\n",
       " 'tigrinya',\n",
       " 'shacket',\n",
       " '63000m',\n",
       " 'herpies',\n",
       " 'mutlicellular',\n",
       " 'relaxtion',\n",
       " 'didm',\n",
       " 'dido',\n",
       " 'didw',\n",
       " 'dids',\n",
       " 'greenwald',\n",
       " 'mewto',\n",
       " 'honduras',\n",
       " '112000',\n",
       " 'honduran',\n",
       " 'indiape',\n",
       " 'rajni',\n",
       " 'psocids',\n",
       " 'lagrangian',\n",
       " 'topcoder',\n",
       " 'macedon',\n",
       " 'palani',\n",
       " 'any911flight',\n",
       " 'digambar',\n",
       " 'anoop',\n",
       " 'arnorld',\n",
       " 'postcolonialists',\n",
       " 'hamel',\n",
       " 'binarysearch',\n",
       " 'hamer',\n",
       " 'howtonotgiveafuck',\n",
       " 'dashnaw',\n",
       " 'fictioned',\n",
       " 'brabourne',\n",
       " 'spokeo',\n",
       " 'bhadotri',\n",
       " 'gsds',\n",
       " 'progrmm',\n",
       " '200usd',\n",
       " 'sedo',\n",
       " 'alecturer',\n",
       " 'usacontact',\n",
       " 'stuterring',\n",
       " 'weisel',\n",
       " 'fuelband',\n",
       " 'someome',\n",
       " 'ventablack',\n",
       " 'ableton',\n",
       " 'deuteride',\n",
       " 'lience',\n",
       " 'narahalli',\n",
       " 'hetchy',\n",
       " 'chandos',\n",
       " 'bannig',\n",
       " 'emem',\n",
       " 'emed',\n",
       " 'eknath',\n",
       " '4762',\n",
       " '1milion',\n",
       " 'wolpert',\n",
       " 'burundi',\n",
       " '4768',\n",
       " 'houlda',\n",
       " 'zod',\n",
       " 'sankar',\n",
       " 'bhagwant',\n",
       " 'mathlete21',\n",
       " 'gettimer',\n",
       " 'littelfuse',\n",
       " 'giuseppe',\n",
       " 'semarang',\n",
       " 'konsus',\n",
       " 'yst',\n",
       " 'exmormons',\n",
       " 'returnship',\n",
       " 'vishwa',\n",
       " 'chandresh',\n",
       " '210000',\n",
       " 'dhakka',\n",
       " 'haauhow',\n",
       " 'heile',\n",
       " 'thangameengal',\n",
       " 'kobject',\n",
       " 'wynk',\n",
       " 'xolotl',\n",
       " 'belluci',\n",
       " 'wonderware',\n",
       " 'gatewayabroad',\n",
       " 'dgmo',\n",
       " 'leatherup',\n",
       " 'carpopedal',\n",
       " 'provigil',\n",
       " 'resaerch',\n",
       " 'hackathone',\n",
       " 'ouchi',\n",
       " 'gradeaundera',\n",
       " 'appalam',\n",
       " 'azoff',\n",
       " 'kodava',\n",
       " 'jinbo',\n",
       " '10mins',\n",
       " 'remustering',\n",
       " 'z51',\n",
       " 'pregrade',\n",
       " 'pimr',\n",
       " 'noroot',\n",
       " 'loews',\n",
       " 'obessive',\n",
       " 'productionrs',\n",
       " 'vohra',\n",
       " 'pimi',\n",
       " 'breslin',\n",
       " 'bluemail',\n",
       " 'multipayer',\n",
       " 'guterres',\n",
       " 'weatherford',\n",
       " 'ampiclox',\n",
       " '832',\n",
       " 'stannis',\n",
       " '831',\n",
       " '835',\n",
       " 'wallmart',\n",
       " 'kues',\n",
       " 'cs157',\n",
       " '839',\n",
       " 'baggrey',\n",
       " 'nrsc',\n",
       " 'bhojpuri',\n",
       " 'tsui',\n",
       " 'abecedarian',\n",
       " 'pandwas',\n",
       " 'linkdin',\n",
       " 'viacom',\n",
       " 'mendeleev',\n",
       " 'tvtropes',\n",
       " 'sggs',\n",
       " 'richfaces',\n",
       " '83x',\n",
       " 'fiu',\n",
       " '83b',\n",
       " 'erlier',\n",
       " 'd3100',\n",
       " 'dream11',\n",
       " 'tadoussac',\n",
       " 'saveetha',\n",
       " 'applicationcontext',\n",
       " 'djkool',\n",
       " 'rajaji',\n",
       " 'e015tc',\n",
       " 'dameron',\n",
       " 'edisha',\n",
       " 'wess',\n",
       " '2posts',\n",
       " 'biafra',\n",
       " 'manterrupting',\n",
       " 'yzr',\n",
       " 'wwan',\n",
       " 'libgdx',\n",
       " 'paedophilia',\n",
       " 'asnwer',\n",
       " 'yzf',\n",
       " 'wilco',\n",
       " 'sowmay',\n",
       " 'haloarene',\n",
       " 'ivec',\n",
       " 'uoh',\n",
       " 'gooduniversities',\n",
       " 'shelob',\n",
       " 'howcan',\n",
       " 'bringbacks',\n",
       " 'broomhilda',\n",
       " 'uom',\n",
       " 'ivey',\n",
       " 'nevile',\n",
       " 'ichangemycity',\n",
       " 'sriram',\n",
       " 'anjar',\n",
       " 'iwatch',\n",
       " 'manyeth',\n",
       " 'inreal',\n",
       " 'g530',\n",
       " 'nanophysics',\n",
       " '7777',\n",
       " 'helpul',\n",
       " 'khamba',\n",
       " 'albatraoz',\n",
       " 'gwp',\n",
       " 'composelabs',\n",
       " 'amrut',\n",
       " 'elina',\n",
       " 'fimo',\n",
       " 'superscore',\n",
       " '1500000ms',\n",
       " 'conversable',\n",
       " 'mpn',\n",
       " '777x',\n",
       " 'shishou',\n",
       " 'jenners',\n",
       " 'exanplae',\n",
       " 'howaboutwe',\n",
       " 'wankhade',\n",
       " 'qbit',\n",
       " 'urma',\n",
       " 'ole55e6v',\n",
       " 'kaveri',\n",
       " 'lucideus',\n",
       " 'bhopali',\n",
       " '11in',\n",
       " 'priuses',\n",
       " 'adema',\n",
       " 'unesco',\n",
       " 'enums',\n",
       " 'decomposer',\n",
       " 'jplt',\n",
       " 'dowloands',\n",
       " 'ioi2017',\n",
       " 'borowitz',\n",
       " 'alibris',\n",
       " 'topologie',\n",
       " 'reeboks',\n",
       " 'escapethecity',\n",
       " 'dilbert',\n",
       " 'samcro',\n",
       " 'mpk',\n",
       " 'heidegger',\n",
       " 'routofy',\n",
       " 'scottie',\n",
       " 'amesim',\n",
       " 'webhook',\n",
       " 'dioxsyn',\n",
       " 'alapacia',\n",
       " 'rudrabhishek',\n",
       " 'xtz125',\n",
       " 'francois',\n",
       " 'flashgot',\n",
       " 'konijn',\n",
       " 'flockof',\n",
       " 'ingrid',\n",
       " 'kjedahls',\n",
       " 'holay',\n",
       " 'percona',\n",
       " 'fmylife',\n",
       " 'soniye',\n",
       " 'soniya',\n",
       " 'nurunatl',\n",
       " 'savilian',\n",
       " '18mm',\n",
       " 'utiltarianism',\n",
       " 'moriarity',\n",
       " '800000',\n",
       " 'expresscard',\n",
       " 'parkash',\n",
       " 'braingasmic',\n",
       " '29250',\n",
       " 'bubbli',\n",
       " 'celcon',\n",
       " 'totoro',\n",
       " 'bfsi',\n",
       " 'abandonado',\n",
       " 'commoncrawl',\n",
       " '269300',\n",
       " 'artical368',\n",
       " 'beyblades',\n",
       " 'dormis',\n",
       " 'fucher',\n",
       " '80000g',\n",
       " 'doordharsan',\n",
       " 'mh370',\n",
       " 'witg',\n",
       " 'sarif',\n",
       " 'satbara',\n",
       " 'uiimage',\n",
       " 'murthys',\n",
       " 'jccsf',\n",
       " 'chromophobia',\n",
       " '397',\n",
       " 'litrerature',\n",
       " 'tripp',\n",
       " 'mechaincal',\n",
       " 'munde',\n",
       " 'vvison',\n",
       " 'jyotiba',\n",
       " 'spreadly',\n",
       " 'nrhm',\n",
       " 'soekarno',\n",
       " '1446',\n",
       " 'hofstra',\n",
       " '1440',\n",
       " 'kumaraguru',\n",
       " '1449',\n",
       " 'reamma',\n",
       " 'circumsized',\n",
       " 'proceducre',\n",
       " 'metacrysis',\n",
       " 'ulhasnagar',\n",
       " 'natalia',\n",
       " 'rrsp',\n",
       " 'natalii',\n",
       " '144v',\n",
       " '144p',\n",
       " 'as911is',\n",
       " 'jtbd',\n",
       " 'ucms',\n",
       " 'rucha',\n",
       " 'hydroquinon',\n",
       " 'aeronotics',\n",
       " 'petrova',\n",
       " 'puneri',\n",
       " 'lcsw',\n",
       " 'norv',\n",
       " 'amaozon',\n",
       " 'october2016',\n",
       " 'assemblylanguage',\n",
       " 'hinsberg',\n",
       " 'dirvert',\n",
       " 'leagel',\n",
       " 'ahmd',\n",
       " 'conputer',\n",
       " 'ehos',\n",
       " 'ehow',\n",
       " 'moines',\n",
       " '0088882578743',\n",
       " 'laproroscopic',\n",
       " 'adine',\n",
       " 'sany',\n",
       " 'insideview',\n",
       " 'rebtel',\n",
       " 'diasteriomers',\n",
       " 'gridview',\n",
       " '199',\n",
       " '198',\n",
       " 'harsha',\n",
       " '195',\n",
       " '194',\n",
       " '197',\n",
       " '196',\n",
       " '191',\n",
       " '190',\n",
       " '193',\n",
       " '192',\n",
       " 'belton',\n",
       " 'strato',\n",
       " 'polae',\n",
       " 'ccnav3',\n",
       " 'blogwriter',\n",
       " 'modline',\n",
       " 'delek',\n",
       " 'ladakh',\n",
       " 'delet',\n",
       " 'leia',\n",
       " 'otou',\n",
       " 'fipb',\n",
       " 'volkawagon',\n",
       " 'sylvestre',\n",
       " 'vvip',\n",
       " '19i',\n",
       " 'gande',\n",
       " 'gandi',\n",
       " '19f',\n",
       " 'trolllo',\n",
       " 'pahree',\n",
       " 'iisuperwomanii',\n",
       " 'lekhe',\n",
       " 'lekha',\n",
       " 'lakshmana',\n",
       " 'fndamntl',\n",
       " 'habra',\n",
       " 'bocelli',\n",
       " 'ilets',\n",
       " 'realplayer',\n",
       " 'scholardhip',\n",
       " 'mohandas',\n",
       " 'vidyarthi',\n",
       " 'kogan',\n",
       " 'dreamspark',\n",
       " '3452',\n",
       " 'cadviewer',\n",
       " 'vlsi',\n",
       " '6qf',\n",
       " 'quanzhou',\n",
       " '3459',\n",
       " 'morw',\n",
       " '35fgh',\n",
       " 'silvy',\n",
       " 'bjts',\n",
       " 'babybel',\n",
       " 'vitz',\n",
       " 'caturmasa',\n",
       " 'protine',\n",
       " 'aricent',\n",
       " 'learm',\n",
       " 'parrondo',\n",
       " '616',\n",
       " 'abrahamic',\n",
       " 'barclay',\n",
       " 'uchihas',\n",
       " 'gallblader',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '/home/ubuntu/quora/'\n",
    "data_home = path +\"data/\"\n",
    "\n",
    "Q1_TRAINING_DATA_FILE = 'q1_train_google.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'q2_train_google.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix_google.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words_google.json'\n",
    "Q1_TESTING_DATA_FILE = 'q1_test_google.npy'\n",
    "Q2_TESTING_DATA_FILE = 'q2_test_google.npy'\n",
    "LABEL_TRAINING_DATA_FILE = data_home+'cache/label_train.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# np.save(open(data_home+\"cache/\"+Q1_TRAINING_DATA_FILE, 'wb'), data_1)\n",
    "# np.save(open(data_home+\"cache/\"+Q2_TRAINING_DATA_FILE, 'wb'), data_2)\n",
    "# # np.save(open(data_home+\"cache/\"+LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
    "# np.save(open(data_home+\"cache/\"+WORD_EMBEDDING_MATRIX_FILE, 'wb'), embedding_matrix)\n",
    "\n",
    "# with open(data_home+\"cache/\"+NB_WORDS_DATA_FILE, 'w') as f:\n",
    "#     json.dump({'nb_words': nb_words}, f)\n",
    "    \n",
    "# np.save(open(data_home+\"cache/\"+Q1_TESTING_DATA_FILE, 'wb'), test_data_1)\n",
    "# np.save(open(data_home+\"cache/\"+Q2_TESTING_DATA_FILE, 'wb'), test_data_2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_1 = np.load(open(data_home+\"cache/\"+Q1_TRAINING_DATA_FILE, 'rb'))\n",
    "data_2 = np.load(open(data_home+\"cache/\"+Q2_TRAINING_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
    "embedding_matrix = np.load(open(data_home+\"cache/\"+WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
    "with open(data_home+\"cache/\"+NB_WORDS_DATA_FILE, 'r') as f:\n",
    "    nb_words = json.load(f)['nb_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_1 = np.load(open(data_home+\"cache/\"+Q1_TESTING_DATA_FILE, 'rb'))\n",
    "test_data_2 = np.load(open(data_home+\"cache/\"+Q2_TESTING_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "#np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_211_120_0.23_0.32\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "\n",
    "print(STAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_model_path = \"weights/lstm_211_120_0.23_0.32.h5\"\n",
    "\n",
    "model.load_weights(bst_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/200\n",
      "727722/727722 [==============================] - 199s - loss: 0.2381 - acc: 0.8231 - val_loss: 0.2126 - val_acc: 0.8634\n",
      "Epoch 2/200\n",
      "727722/727722 [==============================] - 199s - loss: 0.2357 - acc: 0.8257 - val_loss: 0.2191 - val_acc: 0.8599\n",
      "Epoch 3/200\n",
      "727722/727722 [==============================] - 199s - loss: 0.2332 - acc: 0.8273 - val_loss: 0.2205 - val_acc: 0.8576\n",
      "Epoch 4/200\n",
      "727722/727722 [==============================] - 199s - loss: 0.2311 - acc: 0.8296 - val_loss: 0.2178 - val_acc: 0.8498\n",
      "Epoch 5/200\n",
      "727722/727722 [==============================] - 199s - loss: 0.2292 - acc: 0.8309 - val_loss: 0.2163 - val_acc: 0.8524\n"
     ]
    }
   ],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = \"weights/\"+STAMP + '_2.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:qenv]",
   "language": "python",
   "name": "conda-env-qenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
