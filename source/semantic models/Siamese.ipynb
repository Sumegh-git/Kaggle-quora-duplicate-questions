{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 75.0% of memory, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "\n",
    "import utils \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Lambda, merge, BatchNormalization, Activation, Input, Merge\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/quora'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/quora/'\n",
    "data_home = path +\"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_home+\"train.csv\", encoding='utf-8' )\n",
    " \n",
    "# encode questions to unicode\n",
    "# df['question1'] = df['question1'].apply(lambda x: x.encode('utf-8'))# unicode(str(x),\"utf-8\"))\n",
    "# df['question2'] = df['question2'].apply(lambda x: x.encode('utf-8'))#unicode(str(x),\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading pre-trained  GLOVE model which comes free with Spacy. \n",
    "It is trained on Wikipedia and therefore, it is strong in terms of word semantics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'This is a sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = nlp.pipe(df['question1'][1], n_threads=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs1 = [doc.vector for doc in t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'What is the story of Kohinoor (Koh-i-Noor) Diamond?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['question1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vecs1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in t:\n",
    "    print ((doc.vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-455c8a2ac36a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvecs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvecs1\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1_feats'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvecs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/qenv/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, tag, parse, entity, n_threads, batch_size)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/qenv/lib/python2.7/site-packages/spacy/syntax/parser.pyx\u001b[0m in \u001b[0;36mpipe (spacy/syntax/parser.cpp:8294)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mcdef\u001b[0m \u001b[0mint\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mqueue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0mdoc_ptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/qenv/lib/python2.7/site-packages/spacy/matcher.pyx\u001b[0m in \u001b[0;36mpipe (spacy/matcher.cpp:9904)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mDoc\u001b[0m \u001b[0mDocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/qenv/lib/python2.7/site-packages/spacy/syntax/parser.pyx\u001b[0m in \u001b[0;36mpipe (spacy/syntax/parser.cpp:8294)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mcdef\u001b[0m \u001b[0mint\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mqueue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0mdoc_ptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/qenv/lib/python2.7/site-packages/spacy/tagger.pyx\u001b[0m in \u001b[0;36mpipe (spacy/tagger.cpp:6548)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mDoc\u001b[0m \u001b[0mDocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/qenv/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((text,))\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vecs1 = [doc.vector for doc in nlp.pipe(df['question1'], n_threads=200)]\n",
    "vecs1 =  np.array(vecs1)\n",
    "df['q1_feats'] = list(vecs1)\n",
    "\n",
    "vecs2 = [doc.vector for doc in nlp.pipe(df['question2'], n_threads=200)]\n",
    "vecs2 =  np.array(vecs2)\n",
    "df['q2_feats'] = list(vecs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vecs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-267b9dd2cc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvecs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vecs1' is not defined"
     ]
    }
   ],
   "source": [
    "vecs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save features\n",
    "pd.to_pickle(df, data_home+'cache/df_q12_glove_spacy_wiki1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# merge texts\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf.fit_transform(questions)\n",
    "\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF weight normalization\n",
    "\n",
    "After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores. The below code does this for just \"question1\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def normalize_by_tfidf(question):\n",
    "    progress = progressbar.ProgressBar()\n",
    "\n",
    "    normalized_vector = []\n",
    "    idf_zeros = set()\n",
    "    for qu in df[question]:\n",
    "        doc = nlp(qu) \n",
    "        mean_vec = np.zeros([len(doc), 300])\n",
    "        for word in doc:\n",
    "            # word2vec\n",
    "            vec = word.vector\n",
    "            \n",
    "            # fetch df score            \n",
    "            if word2tfidf.has_key(str(word)):\n",
    "                idf = word2tfidf[str(word)]\n",
    "            else:\n",
    "#                 print (word, \", setting idf to 0\")\n",
    "                idf_zeros.add(word)\n",
    "                idf = 0\n",
    "        \n",
    "            # compute final vec\n",
    "            mean_vec += vec * idf\n",
    "        mean_vec = mean_vec.mean(axis=0)\n",
    "        normalized_vector.append(mean_vec)\n",
    "        \n",
    "    return normalized_vector, idf_zeros    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ToDo\n",
    "# df['q1_feats'], idf_zeros1= normalize_by_tfidf('question1')\n",
    "# df['q2_feats'], idf_zeros2= normalize_by_tfidf('question2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: 355775\n",
      "Number of testing pairs: 48515\n"
     ]
    }
   ],
   "source": [
    "# shuffle df\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# set number of train and test instances\n",
    "num_train = int(df.shape[0] * 0.88)\n",
    "num_test = df.shape[0] - num_train                 \n",
    "print(\"Number of training pairs: %i\"%(num_train))\n",
    "print(\"Number of testing pairs: %i\"%(num_test))\n",
    "\n",
    "# init data arrays\n",
    "X_train = np.zeros([num_train, 2, 300])\n",
    "X_test  = np.zeros([num_test, 2, 300])\n",
    "Y_train = np.zeros([num_train]) \n",
    "Y_test = np.zeros([num_test])\n",
    "\n",
    "# format data \n",
    "b = [a[None,:] for a in list(df['q1_feats'].values)]\n",
    "q1_feats = np.concatenate(b, axis=0)\n",
    "\n",
    "b = [a[None,:] for a in list(df['q2_feats'].values)]\n",
    "q2_feats = np.concatenate(b, axis=0)\n",
    "\n",
    "# fill data arrays with features\n",
    "X_train[:,0,:] = q1_feats[:num_train]\n",
    "X_train[:,1,:] = q2_feats[:num_train]\n",
    "Y_train = df[:num_train]['is_duplicate'].values\n",
    "            \n",
    "X_test[:,0,:] = q1_feats[num_train:]\n",
    "X_test[:,1,:] = q2_feats[num_train:]\n",
    "Y_test = df[num_train:]['is_duplicate'].values\n",
    "\n",
    "# remove useless variables\n",
    "del b\n",
    "del q1_feats\n",
    "del q2_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Lambda, merge, BatchNormalization, Activation, Input, Merge\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    '''\n",
    "    Base network for feature extraction.\n",
    "    '''\n",
    "    input = Input(shape=(input_dim, ))\n",
    "    dense1 = Dense(128)(input)\n",
    "    bn1 = BatchNormalization(mode=2)(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "\n",
    "    dense2 = Dense(128)(relu1)\n",
    "    bn2 = BatchNormalization(mode=2)(dense2)\n",
    "    res2 = merge([relu1, bn2], mode='sum')\n",
    "    relu2 = Activation('relu')(res2)    \n",
    "\n",
    "    dense3 = Dense(128)(relu2)\n",
    "    bn3 = BatchNormalization(mode=2)(dense3)\n",
    "    res3 = Merge(mode='sum')([relu2, bn3])\n",
    "    relu3 = Activation('relu')(res3)   \n",
    "    \n",
    "    feats = merge([relu3, relu2, relu1], mode='concat')\n",
    "    bn4 = BatchNormalization(mode=2)(feats)\n",
    "\n",
    "    model = Model(input=input, output=bn4)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    '''\n",
    "    Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n",
    "\n",
    "def create_network(input_dim):\n",
    "    # network definition\n",
    "    base_network = create_base_network(input_dim)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    # because we re-use the same instance `base_network`,\n",
    "    # the weights of the network\n",
    "    # will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    \n",
    "    model = Model(input=[input_a, input_b], output=distance)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "net = create_network(300)\n",
    "\n",
    "# train\n",
    "optimizer = Adam(lr=0.001)\n",
    "net.compile(loss=contrastive_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 355775 samples, validate on 48515 samples\n",
      "Epoch 1/5\n",
      "355775/355775 [==============================] - 16s - loss: 8.4393 - val_loss: 0.2255\n",
      "Epoch 2/5\n",
      "355775/355775 [==============================] - 16s - loss: 0.1967 - val_loss: 0.1856\n",
      "Epoch 3/5\n",
      "355775/355775 [==============================] - 16s - loss: 0.1783 - val_loss: 0.1751\n",
      "Epoch 4/5\n",
      "355775/355775 [==============================] - 16s - loss: 0.1686 - val_loss: 0.1687\n",
      "Epoch 5/5\n",
      "355775/355775 [==============================] - 16s - loss: 0.1615 - val_loss: 0.1638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a44b22c50>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit([X_train[:,0,:], X_train[:,1,:]], Y_train,\n",
    "          validation_data=([X_test[:,0,:], X_test[:,1,:]], Y_test),\n",
    "          batch_size=128, nb_epoch=5, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 355775 samples, validate on 48515 samples\n",
      "Epoch 1/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1555 - val_loss: 0.1596\n",
      "Epoch 2/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1505 - val_loss: 0.1577\n",
      "Epoch 3/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1463 - val_loss: 0.1554\n",
      "Epoch 4/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1431 - val_loss: 0.1533\n",
      "Epoch 5/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1401 - val_loss: 0.1519\n",
      "Epoch 6/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1380 - val_loss: 0.1507\n",
      "Epoch 7/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1364 - val_loss: 0.1523\n",
      "Epoch 8/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1343 - val_loss: 0.1502\n",
      "Epoch 9/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1332 - val_loss: 0.1493\n",
      "Epoch 10/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1316 - val_loss: 0.1496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5975006c50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.optimizer.lr = 1e-4\n",
    "\n",
    "net.fit([X_train[:,0,:], X_train[:,1,:]], Y_train,\n",
    "          validation_data=([X_test[:,0,:], X_test[:,1,:]], Y_test),\n",
    "          batch_size=128, nb_epoch=10, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 355775 samples, validate on 48515 samples\n",
      "Epoch 1/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1304 - val_loss: 0.1480\n",
      "Epoch 2/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1295 - val_loss: 0.1486\n",
      "Epoch 3/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1283 - val_loss: 0.1464\n",
      "Epoch 4/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1276 - val_loss: 0.1466\n",
      "Epoch 5/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1267 - val_loss: 0.1458\n",
      "Epoch 6/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1258 - val_loss: 0.1510\n",
      "Epoch 7/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1252 - val_loss: 0.1464\n",
      "Epoch 8/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1247 - val_loss: 0.1458\n",
      "Epoch 9/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1241 - val_loss: 0.1464\n",
      "Epoch 10/10\n",
      "355775/355775 [==============================] - 16s - loss: 0.1237 - val_loss: 0.1458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a29c9ec90>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.optimizer.lr = 1e-5\n",
    "\n",
    "net.fit([X_train[:,0,:], X_train[:,1,:]], Y_train,\n",
    "          validation_data=([X_test[:,0,:], X_test[:,1,:]], Y_test),\n",
    "          batch_size=128, nb_epoch=10, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.save_weights(path+\"models/siamese_nonorm_nopreproc_.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = net.predict([X_test[:,0,:], X_test[:,1,:]], batch_size=128)\n",
    "te_acc = compute_accuracy(pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7297312782902653"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test = pd.read_csv(data_home+'test.csv')\n",
    "\n",
    " \n",
    "# encode questions to unicode\n",
    "df_test['question1'] = df_test['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "df_test['question2'] = df_test['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "\n",
    "vecs1 = [doc.vector for doc in nlp.pipe(df_test['question1'], n_threads=50)]\n",
    "vecs1 =  np.array(vecs1)\n",
    "df_test['q1_feats'] = list(vecs1)\n",
    "\n",
    "vecs2 = [doc.vector for doc in nlp.pipe(df_test['question2'], n_threads=50)]\n",
    "vecs2 =  np.array(vecs2)\n",
    "df_test['q2_feats'] = list(vecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(df_test, data_home+'cache/test_q12_glove_spacy_wiki1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test['q1_feats'] = vecs1\n",
    "df_test['q2_feats'] = vecs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2345796, 2345796)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test), len(vecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+\"cache/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = net.predict([vecs1, vecs2], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip =0.82 \n",
    "submission_name = \"subm/siamese_nopreproc_glove_1.csv\"\n",
    "flattend = pred.flatten()\n",
    "clipped = np.clip(flattend,1-clip,clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>test_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  test_id\n",
       "0          0.82        0\n",
       "1          0.82        1"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame({'test_id': df_test['test_id'], 'is_duplicate': clipped})\n",
    "sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub.to_csv(path+submission_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/quora/subm/siamese_nopreproc_glove_1.csv' target='_blank'>/home/ubuntu/quora/subm/siamese_nopreproc_glove_1.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/quora/subm/siamese_nopreproc_glove_1.csv"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import FileLink\n",
    "\n",
    "FileLink(path+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:qenv]",
   "language": "python",
   "name": "conda-env-qenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
