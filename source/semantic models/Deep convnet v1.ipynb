{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convnet for each question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "a stacked deep convnet for each individual question similar to VGG for images:\n",
    "\n",
    "```\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, time, json\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation\n",
    "from keras.layers import Conv1D , Flatten, Input\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.convolutional import ZeroPadding1D\n",
    "\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint,EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = '/home/ubuntu/quora/'\n",
    "data_home = path +\"data/\"\n",
    "\n",
    "Q1_TRAINING_DATA_FILE = data_home+'cache/q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = data_home+'cache/q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = data_home+'cache/label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = data_home+'cache/word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = data_home+'cache/nb_words.json'\n",
    "Q1_TESTING_DATA_FILE = 'q1_test.npy'\n",
    "Q2_TESTING_DATA_FILE = 'q2_test.npy'\n",
    "\n",
    "\n",
    "MODEL_WEIGHTS_FILE = path+'weights/conv_weights_v1.h5'\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "NB_EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset, embedding matrix and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\n",
    "q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
    "word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
    "with open(NB_WORDS_DATA_FILE, 'r') as f:\n",
    "    nb_words = json.load(f)['nb_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404290, 35), (404290, 35))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_data.shape,q2_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "# Q1_train = X_train[:,0]\n",
    "# Q2_train = X_train[:,1]\n",
    "# Q1_test = X_test[:,0]\n",
    "# Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [337176  17039  75113 ...,  41001 336218 115328] TEST: [283851 377233  35357 ..., 392228 325350 150065]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "weight_val = np.ones(len(y_test))\n",
    "weight_val *= 0.472001959\n",
    "weight_val[y_test==0] = 1.309028344   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nr_hidden = 200\n",
    "drop_out = 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ConvBlock(x, num_blocks=2, filters=32):\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "#         x = ZeroPadding1D(1)(x)\n",
    "        x = Conv1D(filters, kernel_size=4, padding='valid',activation='relu')(x)\n",
    "        x = BatchNormalization()(x)        \n",
    "        x = Dropout(drop_out)(x)\n",
    "        \n",
    "#     x =  MaxPooling1D(2)(x)\n",
    "    \n",
    "    return x          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/qenv/lib/python2.7/site-packages/ipykernel/__main__.py:97: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.pooling import GlobalMaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "graph_in = Input(shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))\n",
    "x = BatchNormalization()(graph_in)\n",
    "\n",
    "\n",
    "\n",
    "## v4\n",
    "\n",
    "x = ConvBlock(x,2,32)\n",
    "x = ConvBlock(x,2,64)\n",
    "\n",
    "maxpool = GlobalMaxPooling1D()(x)\n",
    "avgpool = GlobalAveragePooling1D()(x)\n",
    "\n",
    "features = concatenate([maxpool,avgpool])\n",
    "\n",
    "x = Dense(nr_hidden, activation='relu')(features)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(drop_out)(x)\n",
    "\n",
    "## v3\n",
    "\n",
    "# x = ConvBlock(x,2,32)\n",
    "# x = ConvBlock(x,2,64)\n",
    "\n",
    "# x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "# x = Dense(nr_hidden)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(drop_out)(x)\n",
    "\n",
    "## v2\n",
    "# x = ConvBlock(x,2,32)\n",
    "# x = ConvBlock(x,2,64)\n",
    "# x = ConvBlock(x,2,128)\n",
    "\n",
    "## v1\n",
    "# x = ZeroPadding1D(1)(x)\n",
    "\n",
    "# the first two layers\n",
    "# x = ZeroPadding1D(1)(graph_in)\n",
    "# x = Conv1D(32, 2, activation='relu')(graph_in)\n",
    "# x = Dropout(0.2)(x)\n",
    "# # x = MaxPooling1D(2)(x)\n",
    "\n",
    "# # x = ZeroPadding1D(1)(graph_in)\n",
    "# x = Conv1D(32, 3, activation='relu')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling1D(2)(x)\n",
    "\n",
    "\n",
    "# deep conv layers\n",
    "# x = ConvBlock(x,2,128)\n",
    "# x = ConvBlock(x,3,256)\n",
    "# x = ConvBlock(x,3,512)\n",
    "# x = ConvBlock(x,3,512)\n",
    "# x = Flatten()(x)\n",
    "\n",
    "stackedConvLayers = Model(inputs=graph_in,outputs=x)\n",
    "\n",
    "\n",
    "### add computation graph to question 1 embeddings\n",
    "\n",
    "Q1 = Sequential()\n",
    "Q1.add(Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False))\n",
    "\n",
    "\n",
    "Q1.add(stackedConvLayers)\n",
    "\n",
    "\n",
    "### Same ops for question 2\n",
    "\n",
    "Q2 = Sequential()\n",
    "Q2.add(Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False))\n",
    "\n",
    "\n",
    "Q2.add(stackedConvLayers)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "# vanilla dense:\n",
    "\n",
    "model.add(Merge([Q1, Q2], mode='concat'))\n",
    "\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(drop_out))\n",
    "\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(drop_out))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='nadam', \n",
    "              metrics=['accuracy'])#, 'precision', 'recall', 'fbeta_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stackedConvLayers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Style distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) +\n",
    "                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_siamese_model():\n",
    "    \n",
    "    graph_in = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    \n",
    "    embedded = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(graph_in)\n",
    "    \n",
    "    \n",
    "    x = BatchNormalization()(embedded)\n",
    "\n",
    "    \n",
    "## v5\n",
    "\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## v4\n",
    "\n",
    "#     x = ConvBlock(x,2,32)\n",
    "#     x = MaxPooling1D()(x)\n",
    "#     x = ConvBlock(x,2,64)\n",
    "#     x = MaxPooling1D()(x)\n",
    " \n",
    "\n",
    "\n",
    "#     x = Flatten()(x)\n",
    "#     maxpool = GlobalMaxPooling1D()(x)\n",
    "#     avgpool = GlobalAveragePooling1D()(x)\n",
    "\n",
    "#     features = concatenate([maxpool,avgpool])\n",
    "\n",
    "    x = Dense(nr_hidden, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "\n",
    "    x = Dense(nr_hidden, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "\n",
    "    return Model(inputs=graph_in,outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.pooling import GlobalMaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "input_a = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "input_b = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "\n",
    "siamese = create_siamese_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Siamese style Euclidean distance:\n",
    "\n",
    "processed_a = siamese(input_a)\n",
    "processed_b = siamese(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "distance = Dense(1, activation='sigmoid')(distance)\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "# vanilla dense:\n",
    "\n",
    "# model.compile(loss=contrastive_loss, optimizer=\"nadam\",metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='nadam', \n",
    "              metrics=['accuracy'])#, 'precision', 'recall', 'fbeta_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# siamese.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model, checkpointing weights with best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2017-05-02 13:55:14.442019\n",
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.4028 - acc: 0.6748 - val_loss: 0.3966 - val_acc: 0.6975\n",
      "Epoch 2/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3583 - acc: 0.7124 - val_loss: 0.3492 - val_acc: 0.7233\n",
      "Epoch 3/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3464 - acc: 0.7241 - val_loss: 0.3429 - val_acc: 0.7132\n",
      "Epoch 4/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3383 - acc: 0.7325 - val_loss: 0.3302 - val_acc: 0.7396\n",
      "Epoch 5/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3324 - acc: 0.7383 - val_loss: 0.3269 - val_acc: 0.7367\n",
      "Epoch 6/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.3272 - acc: 0.7434 - val_loss: 0.3313 - val_acc: 0.7131\n",
      "Epoch 7/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3227 - acc: 0.7471 - val_loss: 0.3221 - val_acc: 0.7391\n",
      "Epoch 8/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3185 - acc: 0.7508 - val_loss: 0.3145 - val_acc: 0.7496\n",
      "Epoch 9/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3143 - acc: 0.7553 - val_loss: 0.3104 - val_acc: 0.7561\n",
      "Epoch 10/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.3104 - acc: 0.7595 - val_loss: 0.3142 - val_acc: 0.7527\n",
      "Epoch 11/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.3080 - acc: 0.7624 - val_loss: 0.3182 - val_acc: 0.7713\n",
      "Epoch 12/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.3045 - acc: 0.7654 - val_loss: 0.3140 - val_acc: 0.7422\n",
      "Epoch 13/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3024 - acc: 0.7672 - val_loss: 0.3066 - val_acc: 0.7572\n",
      "Epoch 14/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.3004 - acc: 0.7702 - val_loss: 0.3043 - val_acc: 0.7566\n",
      "Epoch 15/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.2980 - acc: 0.7732 - val_loss: 0.3049 - val_acc: 0.7573\n",
      "Epoch 16/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.2960 - acc: 0.7744 - val_loss: 0.3010 - val_acc: 0.7607\n",
      "Epoch 17/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.2946 - acc: 0.7759 - val_loss: 0.3000 - val_acc: 0.7714\n",
      "Epoch 18/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.2925 - acc: 0.7785 - val_loss: 0.3047 - val_acc: 0.7541\n",
      "Epoch 19/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.2911 - acc: 0.7794 - val_loss: 0.3019 - val_acc: 0.7631\n",
      "Epoch 20/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.2896 - acc: 0.7806 - val_loss: 0.3012 - val_acc: 0.7691\n",
      "Epoch 21/50\n",
      "363861/363861 [==============================] - 77s - loss: 0.2884 - acc: 0.7819 - val_loss: 0.3013 - val_acc: 0.7637\n",
      "Epoch 22/50\n",
      "363861/363861 [==============================] - 78s - loss: 0.2877 - acc: 0.7829 - val_loss: 0.3002 - val_acc: 0.7676\n",
      "Training ended at 2017-05-02 14:24:25.727983\n",
      "Minutes elapsed: 29.188094\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "MODEL_WEIGHTS_FILE = path+'weights/deepconv_2x32_2x64_gmax_gavg_prepC_fixedweights_epoch_{epoch:02d}_val_loss_{val_loss:.2f}.h5'\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=4)\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_loss', save_best_only=True),early_stopping]\n",
    "\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=256,\n",
    "                    validation_data = ([Q1_test, Q2_test],y_test,weight_val),\n",
    "                    class_weight=class_weight,\n",
    "                    callbacks=callbacks,shuffle=True)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFBCAYAAABuEzZyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUREQuL/AxNopU7ndnhMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd151336050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                    'training': history.history['loss'],\n",
    "                    'validation': history.history['val_loss']})\n",
    "ax = acc.ix[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_ylim([0.0,1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print best validation accuracy and epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without cleaning: Maximum accuracy at epoch 8 = 0.8227\n",
    "with cleaning: Maximum accuracy at epoch 15 = 0.8268\n",
    "with cleaning and stratified: 0.8275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum loss at epoch 35 = 0.2896\n"
     ]
    }
   ],
   "source": [
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print('Maximum loss at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(min_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with best validation accuracy on the test partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "# loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test) #, precision, recall, fbeta_score\n",
    "# print('')\n",
    "# print('loss      = {0:.4f}'.format(loss))\n",
    "# print('accuracy  = {0:.4f}'.format(accuracy))\n",
    "# # print('precision = {0:.4f}'.format(precision))\n",
    "# print('recall    = {0:.4f}'.format(recall))\n",
    "# print('F         = {0:.4f}'.format(fbeta_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(path+\"weights/deepconv_2x32_2x64_gmax_gavg_prepC_fixedweights_epoch_16_val_loss_0.30.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.464873303266\n",
      "0.471671306509\n",
      "0.464492488003\n",
      "0.464592693059\n"
     ]
    }
   ],
   "source": [
    "val_preds = model.predict([Q1_test, Q2_test])\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "print (log_loss(y_test,val_preds))\n",
    "print (log_loss(y_test,np.clip(val_preds,1-0.90,0.90)))\n",
    "print (log_loss(y_test,np.clip(val_preds,1-0.98,0.98)))\n",
    "print (log_loss(y_test,np.clip(val_preds,1-0.99,0.99)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2x64 kernel:2 dropout:0.1\n",
    "0.381922677204\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1_test_data = np.load(open(data_home+\"cache/\"+Q1_TESTING_DATA_FILE, 'rb'))\n",
    "q2_test_data = np.load(open(data_home+\"cache/\"+Q2_TESTING_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 2s, sys: 34.9 s, total: 3min 37s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict([q1_test_data,q2_test_data], batch_size=256)\n",
    "preds_reverse = model.predict([q2_test_data,q1_test_data], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2345796, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = (preds+preds_reverse)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clip =0.98\n",
    "\n",
    "submission_name = \"subm/deep_conv_2x32_2x64_gmax_gavg_fixedweight_reverseaug_c98.csv\"\n",
    "flattend = preds.flatten()\n",
    "\n",
    "clipped = np.clip(flattend,1-clip,clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>test_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  test_id\n",
       "0      0.021083        0\n",
       "1      0.023120        1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(data_home+'test.csv')\n",
    "\n",
    "sub = pd.DataFrame({'test_id': df_test['test_id'], 'is_duplicate': clipped})\n",
    "sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(path+submission_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='subm/deep_conv_2x32_2x64_gmax_gavg_fixedweight_reverseaug_c98.csv' target='_blank'>subm/deep_conv_2x32_2x64_gmax_gavg_fixedweight_reverseaug_c98.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/quora/subm/deep_conv_2x32_2x64_gmax_gavg_fixedweight_reverseaug_c98.csv"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import FileLink\n",
    "\n",
    "FileLink(submission_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:qenv]",
   "language": "python",
   "name": "conda-env-qenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
